# docker-compose.yml - Updated to avoid port 5000 conflict

services:
  vector-db:
    image: qdrant/qdrant:latest
    ports:
      - "6333:6333"
      - "6334:6334"
    volumes:
      - ./data/vectors:/qdrant/storage
    environment:
      - QDRANT_ALLOW_CORS=true
    restart: unless-stopped

  mlflow:
    image: ghcr.io/mlflow/mlflow:latest
    ports:
      - "5001:5000"  # External port 5001 mapped to container's 5000
    volumes:
      - mlflow-docker-data:/mlflow
    environment:
      - MLFLOW_TRACKING_URI=sqlite:///mlflow/mlflow.db
    command: >
      /bin/sh -c "
      mkdir -p /mlflow/artifacts && 
      chmod -R 777 /mlflow && 
      mlflow server 
      --backend-store-uri sqlite:///mlflow/mlflow.db 
      --default-artifact-root /mlflow/artifacts 
      --host 0.0.0.0 
      --port 5000"
    user: root
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "echo", "healthy"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s

  flask-app:
    build:
      context: .
      dockerfile: flask-app/Dockerfile
    ports:
      - "8001:8000"
    volumes:
      - ./data:/flask_app/data
      - ./app:/flask_app/app
      - ./models:/flask_app/models
    environment:
      - FLASK_APP=app.py
      - FLASK_DEBUG=1
      - MLFLOW_HOST=model-server  # Use service name for Docker networking
      - MLFLOW_PORT=5000    # Internal port for Docker networking
      - CMAKE_ARGS=-DLLAMA_CUBLAS=0    # Disable CUDA for llama-cpp-python
      - FORCE_CMAKE=1                  # Force using CMake for building
    depends_on:
      - vector-db
      - mlflow
      - model-server
    restart: unless-stopped

  model-server:
    build:
      context: .
      dockerfile: Dockerfile.model-server
    ports:
      - "5002:5000"
    volumes:
      - ./app:/model_server/app
      - ./models:/model_server/models
      - model-cache:/model_server/.cache
    environment:
      - PORT=5000
      - HF_TOKEN=${HF_TOKEN}
      - HF_MODEL_ID=meta-llama/Llama-3.2-1B-Instruct
      - HF_EMBEDDING_MODEL_ID=sentence-transformers/all-MiniLM-L6-v2
      - HF_RERANKER_MODEL_ID=cross-encoder/ms-marco-MiniLM-L-6-v2
      - MODEL_PATH=/model_server/models/llm/Llama-3.2-1B-Instruct
      - EMBEDDING_MODEL_PATH=/model_server/models/embedding/all-MiniLM-L6-v2
      - RERANKER_MODEL_PATH=/model_server/models/reranker/ms-marco-MiniLM-L-6-v2
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:5000/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s

volumes:
  mlflow-docker-data:
    driver: local
  vector-data:
    driver: local
  model-cache:
    driver: local

networks:
  default:
    driver: bridge
    name: rag-network